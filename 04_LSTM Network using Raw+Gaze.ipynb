{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network using Raw+Gaze\n",
    "Using Recurrent neural network techniques, we trained our model to classify different types of activities performed by a person in a day. Our dataset was collected using a Mobile eye tracker from 10 persons.It consists of the gaze data recorded using eye camera and the scene data recorded using scene camera.We use a sampling frequency of 10 seconds for raw image and gaze data. Then, We generate the optical flow image using the raw images.\n",
    "\n",
    "We split the data into 3 parts. 7 persons for training, 2 persons for testing, 1 person for validation\n",
    "\n",
    "In this model, we use two different modalities of data such as raw image and gaze data as input.\n",
    "\n",
    "This model achieved a hamming loss of 0.2545 and predicted 35 different patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the required library functions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D,Cropping2D,ZeroPadding2D\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D, MaxPool2D\n",
    "from keras.layers.core import Dense, Lambda, Flatten, Reshape, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import telegram\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelegramCallback(Callback):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TelegramCallback, self).__init__()\n",
    "        self.user_id = config['telegram_id']\n",
    "        self.model_name = config['model_name']\n",
    "        self.bot = telegram.Bot(config['token'])\n",
    "\n",
    "    def send_message(self, text):\n",
    "        try:\n",
    "            self.bot.send_message(chat_id=self.user_id, text=text)\n",
    "        except Exception as e:\n",
    "            print('Message did not send. Error: {}.'.format(e))\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        text = 'Start training model {} | {}'.format(self.model.name, self.model_name)\n",
    "        self.send_message(text)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        text = '{} | Epoch {}.\\n'.format(self.model_name, epoch)\n",
    "        for k, v in logs.items():\n",
    "            text += '{}: {:.4f}; '.format(k, v)\n",
    "        self.send_message(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingTensorBoard(TensorBoard):    \n",
    "\n",
    "    def __init__(self, log_dir, settings_str_to_log, **kwargs):\n",
    "        super(LoggingTensorBoard, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "        self.settings_str = settings_str_to_log\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        TensorBoard.on_train_begin(self, logs=logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_PATH = '/home/gururajaramesh/completedata_withP10.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = h5py.File(HDF5_PATH, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17884, 224, 224, 3) (17884, 224, 224, 3) (17884, 60) (17884, 9)\n",
      "(4665, 224, 224, 3) (4665, 224, 224, 3) (4665, 60) (4665, 9)\n",
      "(1381, 224, 224, 3) (1381, 224, 224, 3) (1381, 60) (1381, 9)\n"
     ]
    }
   ],
   "source": [
    "train_x1 = hdf[\"train/rawImages\"]\n",
    "train_x2 = hdf[\"train/flowImages\"]\n",
    "train_x3 = hdf[\"train/gazeData\"]\n",
    "train_y = hdf[\"train/labels\"]\n",
    "\n",
    "\n",
    "test_x1 = hdf[\"test/rawImages\"]\n",
    "test_x2 = hdf[\"test/flowImages\"]\n",
    "test_x3 = hdf[\"test/gazeData\"]\n",
    "test_y = hdf[\"test/labels\"]\n",
    "\n",
    "val_x1 = hdf[\"validation/rawImages\"]\n",
    "val_x2 = hdf[\"validation/flowImages\"]\n",
    "val_x3 = hdf[\"validation/gazeData\"]\n",
    "val_y = hdf[\"validation/labels\"]\n",
    "\n",
    "print(train_x1.shape, train_x2.shape, train_x3.shape, train_y.shape)\n",
    "print(test_x1.shape, test_x2.shape, test_x3.shape, test_y.shape)\n",
    "print(val_x1.shape, val_x2.shape, val_x3.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(set_name, batch_size):\n",
    "    \"\"\"\n",
    "    This generator returns normalized capacitive images (0..1) and the respective labels in mm.\n",
    "    \"\"\"\n",
    "    hdf = h5py.File(HDF5_PATH, \"r\")\n",
    "\n",
    "    pRawImages = hdf[set_name + \"/rawImages\"]\n",
    "    pGazeData = hdf[set_name + \"/gazeData\"]\n",
    "    pLabels = hdf[set_name + \"/labels\"]\n",
    "\n",
    "    len_train = pRawImages.shape[0]\n",
    "    \n",
    "    randomBatchOrder = list(range(len_train-timesteps))\n",
    "       \n",
    "    while True:        \n",
    "        for i in range(0, (len_train // (batch_size))-1):\n",
    "            gaze = []\n",
    "            frames = []\n",
    "            labels = []\n",
    "            for j in range (batch_size):\n",
    "                idx = randomBatchOrder[i*batch_size+j]\n",
    "                shuffled1 = pRawImages[idx : idx+timesteps]\n",
    "                frames.append(shuffled1)\n",
    "                shuffled2 = pGazeData[idx : idx+timesteps]\n",
    "                gaze.append(shuffled2)\n",
    "                shuffled3 = pLabels[idx+timesteps]\n",
    "                labels.append(shuffled3)\n",
    "\n",
    "            yield [np.array(frames).reshape(-1,timesteps,224,224,3)/256, np.array(gaze).reshape(batch_size,-1, 2)], np.array(labels).reshape(-1,9)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "classes = 9 \n",
    "\n",
    "#no.of.epochs\n",
    "epochs = 100 \n",
    "\n",
    "# input image dimensions\n",
    "timesteps, rows, columns, channels = 4, 224, 224, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function for Multilabel Classification\n",
    "def multitask_loss(y_true, y_pred):\n",
    "    # Avoid divide by 0\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # Multi-task loss\n",
    "    return K.mean(K.sum(- y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=32)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=200)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=100)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=9)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 120, 2)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 120, 8)       352         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 120, 8)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4, 224, 224,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 120, 32)      288         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 4, 512)       14714688    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4, 960)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 1472)      0           time_distributed_1[0][0]         \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           393472      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          13000       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          20100       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 9)            909         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,142,809\n",
      "Trainable params: 428,121\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n",
      "RawGaze_v02_Adam_LR001_LSTM8_D32_LSTM64_D200_D100_Sigmoid_withSW20190304_163411\n",
      "Epoch 1/500\n",
      "894/894 [==============================] - 624s 698ms/step - loss: 4.3892 - acc: 0.1900 - val_loss: 3.9432 - val_acc: 0.1019\n",
      "Epoch 2/500\n",
      "894/894 [==============================] - 619s 692ms/step - loss: 4.0416 - acc: 0.1793 - val_loss: 3.8574 - val_acc: 0.1034\n",
      "Epoch 3/500\n",
      "894/894 [==============================] - 628s 702ms/step - loss: 3.6271 - acc: 0.2046 - val_loss: 3.6855 - val_acc: 0.0871\n",
      "Epoch 4/500\n",
      "894/894 [==============================] - 627s 701ms/step - loss: 3.3069 - acc: 0.2535 - val_loss: 3.6661 - val_acc: 0.1105\n",
      "Epoch 5/500\n",
      "894/894 [==============================] - 619s 692ms/step - loss: 3.0805 - acc: 0.2746 - val_loss: 3.7408 - val_acc: 0.1298\n",
      "Epoch 6/500\n",
      "894/894 [==============================] - 627s 701ms/step - loss: 2.9060 - acc: 0.2909 - val_loss: 3.6739 - val_acc: 0.1268\n",
      "Epoch 7/500\n",
      "894/894 [==============================] - 628s 702ms/step - loss: 2.7831 - acc: 0.3079 - val_loss: 3.6897 - val_acc: 0.1311\n",
      "Epoch 8/500\n",
      "894/894 [==============================] - 634s 709ms/step - loss: 2.6701 - acc: 0.3174 - val_loss: 3.5510 - val_acc: 0.1704\n",
      "Epoch 9/500\n",
      "894/894 [==============================] - 629s 704ms/step - loss: 2.5730 - acc: 0.3322 - val_loss: 3.4917 - val_acc: 0.2075\n",
      "Epoch 10/500\n",
      "894/894 [==============================] - 624s 698ms/step - loss: 2.4996 - acc: 0.3443 - val_loss: 3.4046 - val_acc: 0.2090\n",
      "Epoch 11/500\n",
      "894/894 [==============================] - 620s 694ms/step - loss: 2.4176 - acc: 0.3507 - val_loss: 3.3633 - val_acc: 0.2161\n",
      "Epoch 12/500\n",
      "894/894 [==============================] - 623s 696ms/step - loss: 2.3533 - acc: 0.3507 - val_loss: 3.3468 - val_acc: 0.2082\n",
      "Epoch 13/500\n",
      "894/894 [==============================] - 622s 696ms/step - loss: 2.2897 - acc: 0.3559 - val_loss: 3.4419 - val_acc: 0.2058\n",
      "Epoch 14/500\n",
      "894/894 [==============================] - 623s 697ms/step - loss: 2.2366 - acc: 0.3634 - val_loss: 3.3755 - val_acc: 0.2174\n",
      "Epoch 15/500\n",
      "894/894 [==============================] - 624s 698ms/step - loss: 2.1952 - acc: 0.3639 - val_loss: 3.3663 - val_acc: 0.2109\n",
      "Epoch 16/500\n",
      "894/894 [==============================] - 624s 698ms/step - loss: 2.1475 - acc: 0.3694 - val_loss: 3.4074 - val_acc: 0.2174\n",
      "Epoch 17/500\n",
      "894/894 [==============================] - 626s 700ms/step - loss: 2.1162 - acc: 0.3714 - val_loss: 3.4417 - val_acc: 0.2079\n",
      "Epoch 18/500\n",
      "894/894 [==============================] - 626s 700ms/step - loss: 2.0699 - acc: 0.3772 - val_loss: 3.2469 - val_acc: 0.2674\n",
      "Epoch 19/500\n",
      "894/894 [==============================] - 625s 699ms/step - loss: 2.0268 - acc: 0.3799 - val_loss: 3.3029 - val_acc: 0.2633\n",
      "Epoch 20/500\n",
      "894/894 [==============================] - 623s 697ms/step - loss: 2.0090 - acc: 0.3821 - val_loss: 3.3113 - val_acc: 0.2845\n",
      "Epoch 21/500\n",
      "662/894 [=====================>........] - ETA: 2:15 - loss: 1.9645 - acc: 0.3934"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    config = tf.ConfigProto(log_device_placement = True, allow_soft_placement = True)\n",
    "    config.gpu_options.allow_growth=True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    \n",
    "    with tf.Session(config=config):\n",
    "        tf.get_default_graph()    \n",
    "        \n",
    "        #First VGG16 model for Raw RGB Image \n",
    "        RGB_Image = Input(shape=(timesteps, rows, columns, channels))\n",
    "\n",
    "        cnn_base = VGG16(input_shape=(rows,columns,channels), weights=\"imagenet\", include_top=False)\n",
    "\n",
    "        cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n",
    "        cnn = Model(inputs=cnn_base.input, outputs=cnn_out)\n",
    "        for layer in cnn.layers:\n",
    "            layer.trainable=False\n",
    "\n",
    "        encoded_frames1 = TimeDistributed(cnn)(RGB_Image)\n",
    "\n",
    "\n",
    "        #LSTM Layer for extracting features from Gaze data\n",
    "        Gaze_Data = Input(shape=(gazeShape[1], gazeShape[2]))\n",
    "        lstm1 = LSTM(8,activation='relu', return_sequences = True)(Gaze_Data)\n",
    "        lstm1 = Dropout(0.3)(lstm1)\n",
    "        encoded_frames2 = Dense(output_dim=32, activation=\"relu\")(lstm1)\n",
    "        encoded_frames2 = Reshape((timesteps, -1))(encoded_frames2)\n",
    "\n",
    "\n",
    "        #Concatenate the features from the two different modalities\n",
    "        merge = concatenate([encoded_frames1, encoded_frames2])\n",
    "\n",
    "        #Concatenated features are feed to the backend LSTM model for final activity prediction\n",
    "        encoded_sequence = LSTM(64)(merge)\n",
    "        encoded_sequence1 = Dropout(0.3)(encoded_sequence)\n",
    "        hidden_layer1 = Dense(output_dim=200, activation=\"relu\")(encoded_sequence1)\n",
    "        hidden_layer1 = Dropout(0.3)(hidden_layer1)\n",
    "        hidden_layer2 = Dense(output_dim=100, activation=\"relu\")(hidden_layer1)\n",
    "        hidden_layer2 = Dropout(0.3)(hidden_layer2)\n",
    "        outputs = Dense(output_dim=classes, activation=\"sigmoid\")(hidden_layer2)\n",
    "        model = Model([RGB_Image, Gaze_Data], outputs, name ='LSTM_with_1Dense')\n",
    "        model.summary()\n",
    "\n",
    "        #Optimizer for the Neural Network\n",
    "        optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "\n",
    "        #create the tensorboard file\n",
    "        readable_timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "        file_name = \"RawGaze_v02_Adam_LR001_LSTM8_D32_LSTM64_D200_D100_Sigmoid_withSW\" + readable_timestamp\n",
    "        print(file_name)\n",
    "        tensorboardFolder = \"/srv/share/tensorboardfiles/\" + file_name\n",
    "\n",
    "        # create callback\n",
    "        config = {\n",
    "            'token': '685289263:AAEyWyUM3QjljRZjjQVZgIqqLKVPguvj9tg',   # paste your bot token\n",
    "            'telegram_id': 752166506,                                   # paste your telegram_id\n",
    "            'model_name': file_name,\n",
    "        }\n",
    "\n",
    "        #callback for Telegram\n",
    "        tg_callback = TelegramCallback(config)\n",
    "        \n",
    "        #callback for Tensorboard\n",
    "        tb_callback = LoggingTensorBoard(settings_str_to_log=config,\n",
    "                           log_dir=tensorboardFolder,\n",
    "                           histogram_freq=0,\n",
    "                           write_graph=True,\n",
    "                           write_images=True,\n",
    "                           update_freq='epoch'\n",
    "                          )\n",
    "        \n",
    "        #callback for Model Check Point\n",
    "        sm_callback = ModelCheckpoint(str(Path.home()) + \"/models/\"+ file_name + \".{epoch:04d}-{val_loss:.2f}.h5\",\n",
    "                            monitor='val_acc',\n",
    "                            verbose=0,\n",
    "                            save_best_only=True,\n",
    "                            save_weights_only=False\n",
    "                           )\n",
    "\n",
    "        #compile the model\n",
    "        model.compile(loss=multitask_loss,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[\"accuracy\"]) \n",
    "\n",
    "        #fit the compiled model\n",
    "        history = model.fit_generator(myGenerator(\"train\", batch_size),\n",
    "                            steps_per_epoch=len(train_x1) // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            callbacks = [tg_callback, tb_callback, sm_callback],\n",
    "                            verbose=1,\n",
    "                            validation_data=myGenerator(\"test\", batch_size),\n",
    "                            validation_steps=len(test_x1) // batch_size)\n",
    "        \n",
    "        #predict using the trained model with the validation data\n",
    "        result = model.predict_generator(myGenerator(\"validation\", batch_size), steps=len(val_x1) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for Training vs Test Accuracy\n",
    "plt.plot(history.history['val_acc'], label=\"Test Accuracy\")\n",
    "plt.plot(history.history['acc'], label=\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot for Training vs Test Loss\n",
    "plt.plot(history.history['val_loss'], label=\"Test Loss\")\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training vs Test Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain predicted output in terms of one hot coding\n",
    "y_predict = np.copy(result)\n",
    "y_predict[y_predict>0.5]=1\n",
    "y_predict[y_predict<0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 1, 9)\n",
      "(1381, 9)\n",
      "(1380, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.load('/home/gururajaramesh/P10data/LabelsP10.npy')\n",
    "print(y_true.shape)\n",
    "y_true = y_true.reshape(-1,9)\n",
    "print(y_true.shape)\n",
    "y_true1= y_true[0:1380,:]\n",
    "print(y_true1.shape)\n",
    "y_true1[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Summary of the precision, recall, F1 score for each class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.22      0.47      0.30       249\n",
      "           2       0.80      0.19      0.31       803\n",
      "           3       0.06      0.08      0.07        79\n",
      "           4       0.80      0.29      0.43       897\n",
      "           5       0.06      0.12      0.08       121\n",
      "           6       0.51      0.15      0.23       202\n",
      "           7       0.08      0.02      0.03       136\n",
      "           8       0.14      0.20      0.17       128\n",
      "\n",
      "   micro avg       0.36      0.23      0.28      2702\n",
      "   macro avg       0.30      0.17      0.18      2702\n",
      "weighted avg       0.58      0.23      0.29      2702\n",
      " samples avg       0.28      0.22      0.23      2702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "#Classfication report for the predicted output\n",
    "print ('\\n Summary of the precision, recall, F1 score for each class:')\n",
    "print (sklearn.metrics.classification_report(y_true1, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hamming Loss: \n",
      "0.2545893719806763\n"
     ]
    }
   ],
   "source": [
    "#Hamming loss of the predicted output\n",
    "print ('\\n Hamming Loss: ')\n",
    "print(sklearn.metrics.hamming_loss(y_true1, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found new pattern : [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.] pattern found 130 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.] pattern found 36 times\n",
      "Found new pattern : [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.] pattern found 74 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.] pattern found 127 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.] pattern found 143 times\n",
      "Found new pattern : [0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 1. 0. 0. 0. 0.] pattern found 118 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 1.] pattern found 1 times\n",
      "Found new pattern : [0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 1.] pattern found 14 times\n",
      "Found new pattern : [0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 0.] pattern found 20 times\n",
      "Found new pattern : [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.] pattern found 79 times\n",
      "Found new pattern : [0. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
      "[0. 0. 1. 0. 1. 0. 1. 0. 0.] pattern found 10 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.] pattern found 9 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 1. 0.] pattern found 3 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 1. 0. 0.] pattern found 2 times\n",
      "Found new pattern : [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.] pattern found 446 times\n",
      "Found new pattern : [0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 0.] pattern found 3 times\n",
      "Found new pattern : [0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 0.] pattern found 1 times\n",
      "Found new pattern : [0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0.] pattern found 13 times\n",
      "Found new pattern : [0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 1.] pattern found 10 times\n",
      "Found new pattern : [0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 0.] pattern found 1 times\n",
      "Found new pattern : [0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 1. 1. 0. 0. 0.] pattern found 42 times\n",
      "Found new pattern : [1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 0.] pattern found 2 times\n",
      "Found new pattern : [0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 0.] pattern found 2 times\n",
      "Found new pattern : [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 0. 0. 0. 0.] pattern found 16 times\n",
      "Found new pattern : [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 1. 0.] pattern found 23 times\n",
      "Found new pattern : [0. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0. 0.] pattern found 13 times\n",
      "Found new pattern : [0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 1.] pattern found 14 times\n",
      "Found new pattern : [0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 1. 0. 0.] pattern found 5 times\n",
      "Found new pattern : [0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 1. 1. 0. 1. 0. 0. 0. 0.] pattern found 5 times\n",
      "Found new pattern : [0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "[0. 0. 1. 0. 1. 0. 0. 0. 1.] pattern found 12 times\n",
      "Found new pattern : [0. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 1.] pattern found 1 times\n",
      "Found new pattern : [0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 1. 0. 0.] pattern found 1 times\n",
      "Found new pattern : [0. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
      "[0. 1. 1. 0. 1. 0. 0. 0. 1.] pattern found 1 times\n",
      "Found new pattern : [0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 1. 1. 0.] pattern found 1 times\n",
      "Found new pattern : [0. 1. 1. 0. 1. 1. 0. 0. 0.]\n",
      "[0. 1. 1. 0. 1. 1. 0. 0. 0.] pattern found 1 times\n",
      "Found new pattern : [0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "[0. 1. 0. 0. 1. 0. 0. 0. 1.] pattern found 1 times\n"
     ]
    }
   ],
   "source": [
    "#to find the number of different patterns in the predicted output\n",
    "y_true = np.copy(y_predict)\n",
    "firsttime = True\n",
    "patternlist = []\n",
    "countlist = []\n",
    "for i in range(y_true.shape[0]):\n",
    "    if(firsttime):\n",
    "        curr_pattern = y_true[0,:]\n",
    "        print('Found new pattern : %s' % curr_pattern)\n",
    "        patternlist.append(curr_pattern)\n",
    "        count = 0\n",
    "        for j in range(y_true.shape[0]):\n",
    "            #check if the pattern is same as the curr.pattern and increase count if not continue\n",
    "            if(np.array_equal(curr_pattern, y_true[j,:])):\n",
    "                count =  count + 1\n",
    "            else:\n",
    "                continue\n",
    "        countlist.append(count)\n",
    "        firsttime = False\n",
    "        print('%s pattern found %s times' % (curr_pattern, count))\n",
    "    else:\n",
    "        curr_pattern = y_true[i,:]\n",
    "        new_pattern = False\n",
    "        already_present= False\n",
    "        #check if curr_pattern is in patternlist\n",
    "        for k in range(len(patternlist)):\n",
    "            if(np.array_equal(curr_pattern, patternlist[k])):\n",
    "                already_present = True\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        if(already_present):\n",
    "            new_pattern = False\n",
    "        else:\n",
    "            new_pattern = True\n",
    "        #if it is a new pattern  then count its occurance    \n",
    "        if(new_pattern):\n",
    "            print('Found new pattern : %s' % curr_pattern)\n",
    "            patternlist.append(curr_pattern)\n",
    "            count = 0\n",
    "            for j in range(y_true.shape[0]):\n",
    "                if(np.array_equal(curr_pattern, y_true[j,:])):\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    continue\n",
    "            countlist.append(count)\n",
    "            print('%s pattern found %s times' % (curr_pattern, count))\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the No.of Samples vs Types of patters\n",
    "x1 = list(range(len(countlist)))\n",
    "plt.bar(x1, countlist)\n",
    "plt.xlabel('Types of Patterns', fontsize=10)\n",
    "plt.ylabel('No. of Samples', fontsize=10)\n",
    "plt.title('Long Term Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the HDF5 file\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model as HDF5 file\n",
    "model.save(\"Raw+Gaze_Adam.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
